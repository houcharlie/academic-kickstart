+++
title = "Notes on setup for DMDP problems"
date = 2018-10-08T00:00:00

# List format.
#   0 = Simple
#   1 = Detailed
#   2 = Stream
list_format = 2

# Optional featured image (relative to `static/img/` folder).
[header]
image = ""
caption = ""
math = true
summary = " "
+++

These are some notes I've compiled over the course of reading [this paper](https://arxiv.org/pdf/1806.01492.pdf), which was published recently by Sidford, Wang et al.  I came across this paper at the suggestion of one of the authors in the paper, who is a [professor](http://www.princeton.edu/~mengdiw/) in my department.  I want crystalize my thoughts to test and record my understanding, but maybe this will be useful to others trying to get started in RL research.

This only contains the basic preliminaries.  These preliminaries will be updated as necessary as sort of a glossary, and are assumed to be the same for other blog posts on the topic.  I wanted to keep these blog posts short and to the point.  This way, hopefully each post can really drive home its main idea.


## The typical framework studied in reinforcement learning

Let $$\cal{S}$$ be the set of all states, let $$\cal{A}$$ be the set of all actions that can be taken from any state.  

Then let $$ v $$ be a the value vector.  $$ v_{i} $$ represents the expected payoff if we enter state $$ i $$.  Let $$ P $$ be the state-action-state transition matrix, which is essentially a tensor.  Think of it as a $$ \|\cal{S}\|\times\|\cal{A}\| $$ matrix, with a vector of length $$ \|\cal{S}\| $$ in every entry. We assign in $$ P_{s,a,s'} $$ the probability that we go to state $$ s' $$ if we take action $$ a $$ from state $$ s $$.

Let $$ r_{s,a} $$ be the reward for taking action $$ a $$ at state $$ s $$, and assume each reward is between 0 and 1.  Now note that $$Pv$$ is a $$ \|\cal{S}\|\times\|\cal{A}\| $$ matrix, where each entry is $$ P_{s,a}^{T}v $$.  If you think about it, $$ P_{s,a}^{T}v $$ represents the expected payoff after taking action $$ a $$ from $$ s $$ (just write out the dot product and it becomes clear).

Now, what we want to do is to come up with a policy $$ \pi $$, which is a function that takes in a state and spits out the action, defined over all states.  The best policy assigns actions to states in such a way that maximizes the reward over the entire process.  We call this policy $$ \pi^{*} $$.  
Call $$ T(v)_{s} = max_{a \in \cal{A}} {r_{s,a} + \gamma P_{s,a}^{T}v} $$ the value operator on a given value vector.  $$\gamma \in (0,1)$$ here represents the discount factor, or amount we discount future value.  Note that the function returns another vector.  The interpretation here is that the value of a state is equal to the reward from taking that action at the current state plus the expected payoff after taking that state (Note $$P_{s,a}^{T}v$$, as stated earlier, is the expected payoff after taking action a from state s) times the discount factor, which decreases the future payoff by the factor specified.  
We define the value of the optimal policy as $$ T(v^{*}) = v^{*} $$.  The reason is that the maximizer of the function is the fixed point.
Call $$T_{\pi}(v) = r_{s,\pi(s)} + \gamma P_{s,\pi(s)}^{T}v $$ as the value operator associated with $$\pi$$.  Note that this definition is the same as the one above but no maximum is taken, as $$a$$ is determined.  The value associated with $$\pi$$ is defined as $$v^{\pi}$$ where $$T_{\pi}(v^{\pi}) = v^{\pi}$$
A value $$v$$ is $$\epsilon$$-optimal if $$\|v^{*} - v\|_{\infty} \leq \epsilon$$, and a policy is $$\epsilon$$-optimal if $$\|v^{*} - v^{\pi}\|_{\infty} \leq \epsilon$$ Define $$Q^{\pi}(s,a) = r_{s,a} + \gamma P^{T}_{s,a}v^{\pi}$$ as the $$Q$$-function wrt $$\pi$$.  Note that $$Q^{\pi}(s, \pi(s)) = T_{\pi}(v^{\pi}) = v^{\pi}$$.  So it both encodes the best payoffs for $$\pi$$ as well as the less than optimal payoffs if the wrong action is taken given $$v^{\pi}$$ as the expected future payoffs.

The optimal $$Q$$-function is $$Q^{*} = Q^{\pi^{*}}$$.  While the $$Q$$ function is a function, it is also somewhat useful to think of it as a fixed matrix, as it may be defined independently of $$\pi$$.  

Define for all $$s$$ $$v(Q)(s) = max_{a}Q(s,a)$$ and $$\pi(Q)(s) = argmax_{a} Q(s,a)$$.  

In words, $$v(Q)$$ is the vector of the best payoff from each state s, and $$\pi(Q)$$ is the action to get the best payoff for each state s.




