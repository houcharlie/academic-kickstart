+++
title = "Interpolated Peeling-New learning algorithms for vertex order recovery"
date = 2018-10-11T00:00:00

# List format.
#   0 = Simple
#   1 = Detailed
#   2 = Stream
list_format = 2

# Optional featured image (relative to `static/img/` folder).
[header]
image = ""
caption = ""
math = true
summary = ""
+++

Imagine that there is some seed graph.  Then, nodes are added to the graph one by one, edges forming between each new node and the original graph according to some random process.  Recovering the order in which the nodes came in, given some final fixed graph, is an important research problem.  For example, being able to do this would allow us to trace the origination of fake news in a social network.  In general, when there is a network, it often is of interest to know how the network evolved.  

We decided to study vertex order in the context of the [Barabasi-Albert model](https://en.wikipedia.org/wiki/Barab%C3%A1si%E2%80%93Albert_model), which essentially says that each new node has a probability of attaching to each node with a probability proportional to the number of edges the target node has.  

An algorithm was proposed [here](https://pdfs.semanticscholar.org/043a/4b15b8f563002e1d1e3ee8dea5eed9aa26ca.pdf) to solve this problem, and learn the order in which the nodes arrived.  The algorithm is as follows:

1. Find all the nodes with the lowest degree  
2. Place them into a bin.  These are the "youngest" vertices  
3. Remove these vertices from the graph  
4. Repeat from step one, except the next bin is the next "youngest"  

They call this the peeling algorithm.  They also introduce a second algorithm, the peeling+ algorithm.  First, it uses peeling to produce a binning.  Then it looks at the original graph, and looks at the average "age" of the vertices that each vertex is connected to, given the binning.  Then it uses this to tiebreak the nodes in each bin (as the original binning algorithm treats all nodes in a bin as having the same age).

Note that this problem cannot be solved perfectly, as a given graph could have been generated in a number of different ways.  For example, say you start with one node.  Then you add one node to the graph.  This gives you v-v'.  How can you tell which is younger, given this configuration?  You cannot.

The authors of that paper proved that peeling is "optimal", in the sense that it gets the correct order for all pairs of vertices that it "should get correct".  "Should get correct" pairs are pairs that must always be in the same order given the configuration of the graph.  So this algorithm is optimal in the worst-case sense.

There are a lot of questions that arise from this.  First, the peeling algorithm often performs better than the worst case.  How do we describe that?  Second, there is a peeling and peeling+, but what if we want to be able to tune the algorithm such that we get finer control over how much to tiebreak (more tiebreaking means more guesses and less accuracy).  What is the general process by which the algorithm proceeds?  We attempt to answer these questions, and this blog post will only address them incompletely.

### A new algorithm: Interpolated Peeling

To answer the second question, we introduce a new algorithm named Interpolated Peeling.  The code to the algorithm is [here](https://github.com/houcharlie/peelingAnalysis/blob/master/R%20Scripts/interPeel/dualGenPeel.R).  We first perform the peeling algorithm, and separate the nodes into their bins.  Then, we tiebreak the nodes like in the peeling+ algorithm, except we only tiebreak them if the difference in their neighbors' ages is significant enough.  Significant enough is defined as the age being greater than a proportion set by the user times the mean of the neighbors' ages in the bin.  We could have also used a z-score based thresholding, but it mostly gave the same results, and the code is cleaner this way.

Let precision be the proportion of guesses the algorithm gets correct out of the total number of guesses and recall be the proportion of correct guesses out of the total number of pairs of vertices.

<p style="text-align:center;">
<img src="https://houcharlie.github.io/static/img/gen_vs_all_m10_t90.png" width="300" alt="Interpolated">
</p>  
The measures prefixed with ”Gen” are the ones made from interpolated peeling. Prefix "orig" means the peeling algorithm.  Prefix "aug" means the peeling+ algorithm.  Note how interpolated peeling has precision and recall between peeling and peeling+.  This is a graph comparing precision and recall between the three algorithms at different numbers of nodes.  

Remark: As the threshold becomes more stringent, the interpolated peeling algorithm gets closer to peeling.  As the threshold becomes more lax, the interpolated peeling algorithm converges to peeling+.

It's clear from this graph that interpolated peeling is a middle ground between peeling and peeling+.

We also did some analysis on how precision and recall vary with changing the threshold.

<p style="text-align:center;">
<img src="https://houcharlie.github.io/static/img/bound_m1_n100.png" width="300" alt="Interpolated bound">
</p> 

This is a graph of the precision and recall for interpolated peeling algorithms at different thresholds.  This is actually 100 different instantiations of the algorithm at 100 different thresholds.  Many of the values cluster together, and we don't get a clean line.  However, it is apparent there is a linear relationship here.  Why can't we access certain values and why does the relationship appear linear?

In light of this mystery, we investigate the algorithm both experimentally and theoretically in the later sections.


### Aside: A new algorithm that violates the bin structure
The derivative algorithms we created from binning ultimately don't allow for switching between bins.  One might think that by using the neighbors' ages, we might want to move a vertex to an older bin if its neighbors' ages are really high.  The code to the algorithm is [here](https://github.com/houcharlie/peelingAnalysis/blob/master/R%20Scripts/exchangePeel/exchPeel.R).  The results are in the graph below.

<p style="text-align:center;">
<img src="https://houcharlie.github.io/static/img/exch_m10_n1000.png" width="300" alt="Exchange bound">
</p> 

We basically used the peeling algorithm, then used the neighbors' ages to move vertices with outlier neighbor ages up or down bins.  The red dot is the peeling algorithm, and the line denotes all the different variants of this "exchange algorithm" based on different thresholds for "outlier".  As we can see, the peeling algorithm strictly outperforms all algorithms of this type.  Thus, we think violating the bin gives suboptimal results.



## "Peeling back" the layers of mystery

In this section, I describe the work we did in understanding how the peeling algorithm behaves.  We hope that giving a more complete characterization will help shed light on the mysteries listed above.


### Understanding the evolution of binning on a micro scale
First, we view the binning process as a dynamic process.  By this I mean that any fixed graph has a binning that is also fixed.  So when we evolve a graph through a random process, the binning also evolves.  We try to study the way the binning evolves through time.  

Studying it in this way, we come to the main theoretical result from the report:

Lemma 1:  
(1) if $$v \in B_{i}^{(t)}$$ (where $$B_{i}^{(t)}$$ is the ith oldest bin at the t'th time step of the graph) is part of a bin path (a path that connects the vertex to the youngest bin), and the incoming vertex in the next time step forms an edge with the end of the path then $$v \in B_{i + 1}^{(t+1)}$$.  
(2) if $$v$$ is not part of a bin path that has the incoming vertex form an edge with the end of it, it cannot move to an older bin.  

Taken together, this means that a vertex moves up in the binning iff the vertex is connected to a vertex in the youngest bin that forms an edge with an incoming vertex.  This gives us a precise characterization of the mechanism by which the binning evolves.


### Understanding the evolution of binning on a macro scale
To further understand the binning process, we also viewed it on a wider scale to get a sense of the trends.

First, the distribution of bin sizes tends to be exponential:

<p style="text-align:center;">
<img src="https://houcharlie.github.io/static/img/binsizes.png" width="300" alt="Bin sizes">
</p> 

And the edge distributions also tend to be exponential (the edge distributions being of how many bins up are vertices in a particular bin neighbors with).

<p style="text-align:center;">
<img src="https://houcharlie.github.io/static/img/edgeDistm1.png" width="300" alt="Bin sizes">
</p> 

This is a log scaled graph of the edge distributions of vertices in the first bin generated n = 10000
and m = 1 (m is the number of edges an incoming node forms with existing nodes in the graph). This is averaged over 50 graphs. x-axis is how many bins up the edges are and y-axis is how
many edges satisfy this. Note that this is linear away from the beginning and ends of the graph.

<p style="text-align:center;">
<img src="https://houcharlie.github.io/static/img/edgeDist20.png" width="300" alt="Bin sizes">
</p> 
This is a log scaled graph of the edge distributions of vertices in bin 6 generated n = 10000 and m
= 5. This is averaged over 50 graphs. x-axis is how many bins up the edges are and y-axis is how many
edges satisfy this. Note that this is linear away from the beginning and ends of the graph.

We conjecture that both the edge distribution and the bin distribution are exponential with high probability.  This makes (some) intuitive sense.  If a node has some probability $$c$$ of being neighbors with a node in a bin above it, then it roughly has probability $$c^{n}$$ of being connected to the nth oldest bin.  This approximately exponential relationship should underly most of what we see in the graphs above.

Understanding and characterizing these graphs precisely will help us understand the mystery of interpolated peeling better, as we will be able to characterize the bin paths better and understand the peeling process's evolution better.  Then by understanding that evolution, we can hopefully understand interpolated peeling's evolution, and give some answers for the questions surrounding the interpolated peeling algorithm's performance.


### Discussion

Is there an elegant and precise way to describe peeling and its variants?  Can we find a way to change interpolated peeling so we have better control over precision and recall?  Finally, some food for thought: if we can assume that two graphs had the same order in which the vertices came in, could we possibly perform better?  What if we had $$n$$ graphs?  It seems that if we did have that, we could construct an algorithm that could allow us to predict node arrivals much easier.  But is there an application for such a problem?  Are there cases in the real world where we might see the same order of objects form multiple graphs?





